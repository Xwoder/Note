[TOC]

# 信息熵

在机器学习和信息论中，**信息熵（Entropy）**是衡量**随机变量不确定性**的一种度量。信息熵的概念最早由**克劳德·香农（Claude Shannon）**在1948年提出，它用于量化信息的平均不确定度。

---

## 1. **信息熵的数学定义**
给定一个离散随机变量 $$X$$，其可能的取值为 $$ x_1, x_2, \dots, x_n $$，对应的概率分布为 $$ P(X) = \{p_1, p_2, \dots, p_n\} $$，那么**信息熵**定义为：

$$
H(X) = - \sum_{i=1}^{n} p_i \log_2 p_i
$$

其中：
- $$ H(X) $$ 表示随机变量 $$ X $$ 的熵；
- $$ p_i $$ 是随机变量 $$ X $$ 取值 $$ x_i $$ 的概率；
- $$\log_2 p_i$$ 表示以 2 为底的对数（也可以使用自然对数，底为 $$ e $$，此时单位为 Nat）。

信息熵的单位通常使用**比特（bit）**（若底为 2）或**纳特（nat）**（若底为 e）。

---

## 2. **信息熵的直观理解**
- **熵越大，表示随机变量的不确定性越大，信息量越多。**
- **熵越小，表示随机变量的不确定性越小，信息量越少。**
- 如果一个事件总是确定发生（即 $$ p=1 $$），则熵为 0，因为没有不确定性。

举个例子：
- **掷硬币：** 若是公平的硬币， $$ P(正面) = 0.5, P(反面) = 0.5 $$，则：

  $$
  H(X) = - (0.5 \log_2 0.5 + 0.5 \log_2 0.5) = 1 \text{ bit}
  $$

  说明此时的不确定性较高，每次投掷都会带来 1 bit 的新信息。

- **掷硬币（非公平）：** 若 $$ P(正面) = 0.9, P(反面) = 0.1 $$，则计算得到 $$ H(X) \approx 0.47 \text{ bit} $$，即熵比公平硬币低，因为大多数情况下可以预测正面朝上。

- **确定性事件：** 若 $$ P(A) = 1 $$，则 $$ H(X) = 0 $$，因为没有不确定性，不会获得任何新信息。

---

## 3. **信息熵在机器学习中的应用**
### (1) **决策树（Decision Tree）**
在决策树（如 ID3、C4.5、CART）中，信息熵用于衡量数据集的纯度：
- 熵越高，数据分布越混乱，类别越不确定；
- 熵越低，数据分布越集中，类别越确定。

决策树通过**信息增益（Information Gain）**来选择最优划分属性：

$$
\text{信息增益} = H(\text{划分前}) - H(\text{划分后})
$$

熵越降低越多，说明划分后的数据更加纯净，是较优的划分方式。

### (2) **交叉熵（Cross Entropy）**
在分类任务（如深度学习中的 Softmax + 交叉熵损失）中，交叉熵用于衡量预测分布 $$ p $$ 和真实分布 $$ q $$ 之间的差异：

$$
H(p, q) = - \sum_{i} q_i \log p_i
$$

它常用于深度学习的损失函数，如**二分类交叉熵损失**和**多分类交叉熵损失**。

### (3) **最大熵模型（Maximum Entropy Model）**
最大熵原理认为，在满足已知约束的情况下，应选择熵最大的概率分布，以保证对未知信息的假设最小化。它常用于**自然语言处理（NLP）**等任务。

---

## 4. **总结**
- **信息熵衡量随机变量的不确定性，熵越大，数据越混乱，熵越小，数据越纯净。**
- 在**决策树**中，信息熵用于选择最优划分属性（信息增益）。
- 在**分类问题**中，交叉熵用于计算预测分布与真实分布的差异。
- **最大熵模型**在 NLP 领域广泛应用。

信息熵是机器学习中的重要概念，它帮助我们理解数据的不确定性，并在多个算法中发挥核心作用！